{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEI Conference 2019 #\n",
    "\n",
    "## Etape 4 : Classification non supervisée ##\n",
    "\n",
    "### Préambule ###\n",
    "\n",
    "Nous allons faire ici de la classification non supervisée grâce au package SKLearn, librairie dédiée au Machine Learning.\n",
    "\n",
    "L'objectif est de soumettre librement le corpus à l'ordinateur et de le laisser créer des clusters, c'est à dire des regroupements de textes. Pour cela, l'ordinateur va vectoriser chaque terme de chaque abstract du corpus, permettant d'attribuer à chaque mot une valeur unique pondérée. Grâce à ces calculs, nous obtenons une représentation des textes dans l'espace, que l'ordinateur peut ensuite réunir en calculant la proximité de chacun des vecteurs / textes depuis certains points (les centroids) disposés aléatoirement. La répétition de cette expérience aléatoire permet de définir une classification supposée objective.\n",
    "\n",
    "Pour la vectorisation, nous utilisons la méthode TF-IDF de SKLearn. La TF-IDF (Term Frequency-Inverse Document Frequency) est une méthode de calcul de la valeur attribuée à chaque mot qui se différencie d'abord de la fréquence brut (compte du nombre d'occurence) et de la fréquence relative (compte du nombre d'occurence divisé par le nombre de mots du texte) **en divisant la fréquence d'un mot par le nombre de corpus où ce mot est présent**.\n",
    "Cette méthode a l'avantage de pouvoir résoudre un problème dans la fouille de texte : il a été effectivement remarqué qu'un terme fréquent dans un texte est généralement fréquent également dans les autres documents du corpus. Ainsi, **la TF-IDF permet de relever des termes qui sont fréquents distinctement dans un texte au regard des autres textes du corpus**, et non plus simplement des termes fréquents dans un texte. Nous pouvons alors distinguer un texte des autres, et rassembler des textes ensemble.\n",
    "\n",
    "La classification que nous obtiendrons nous permettra de comparer avec le plan déterminé par le programme de la conférence TEI 2019, par exemple.\n",
    "\n",
    "### Les packages ### \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut lancer la cellule ci-dessous pour importer les packages dédiés.\n",
    "\n",
    "La librairie SKLearn permet, grâce à la TF-IDF, de vectoriser les textes, c'est à dire d'attribuer à chaque mot un vecteur unique le caractérisant dans l'espaces. KMeans permet en outre de définir des clusters.\n",
    "\n",
    "La librairie spaCy permet de lemmatiser, tandis que la librairie PorterStemmer de NLTK permet de stemmer.\n",
    "\n",
    "Les librairies Pandas, Numpy et Scipy permettent de traiter la donnée et de produire des tableaux à partir des textes vectorisés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# Vectorisation\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "#Pour la représentation des clusters dans la partie graphique\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "#Pour une ébauche de topic modeling à partir de NMF\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choisir son corpus de travail ###\n",
    "\n",
    "Ensuite, il faut choisir quel set sera loadé et analysé entre les abstracts lemmatisés et les abstracts loadés. Le résultat change entre les deux versions. Pour un premier essai, il est premier de travailler avec la version lemmatisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour utiliser les textes stemmés, c'est ici\n",
    "\n",
    "documents = []\n",
    "Path = \"./cache2019/cacheSTEM/\"\n",
    "filelist = os.listdir(Path) #filelist est une liste regroupant tous les chemins vers les différents abstracts.\n",
    "\n",
    "for abstract in filelist:\n",
    "    with open(Path + abstract, \"r\", encoding=\"UTF-8\") as y:\n",
    "        texte = y.read()\n",
    "        documents.append(texte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour utiliser les textes lemmatisés, c'est ici\n",
    "\n",
    "documents = []\n",
    "Path = \"./cache2019/cacheLEM/\"\n",
    "filelist = os.listdir(Path) #filelist est une liste regroupant tous les chemins vers les différents abstracts.\n",
    "\n",
    "for abstract in filelist:\n",
    "    with open(Path + abstract, \"r\", encoding=\"UTF-8\") as y:\n",
    "        texte = y.read()\n",
    "        documents.append(texte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Définir les clusters ##\n",
    "\n",
    "Après avoir tokennisé et lemmatisé/stemmé nos abstracts, nous allons maintenant les vectoriser (représenter chaque mot sous la forme d'un vecteur sur un plan ordonné en deux dimensions) puis ensuite nous allons laisser la machine définir des clusters, c'est à dire des groupes de mots qui ont un sens commun.\n",
    "\n",
    "Ici, on associe à chaque mot de tous les abstracts une coordonnée unique à chaque mot utilisé, pour ensuite pouvoir les placer sur un plan en 2D et ainsi relever des ressemblances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/probert5/Documents/duranty_text_analyze/traitement/Devoir_Python/venv/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['qu', 'quelqu'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "final_stopwords_list = list(fr_stop)\n",
    "vectorizer = TfidfVectorizer(stop_words=final_stopwords_list)\n",
    "X = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, on réunit les termes vectorisés en cluster, c'est à dire en groupe de ressemblance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_samples=1 should be >= n_clusters=2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-a164d1caef64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrue_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrue_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k-means++'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/duranty_text_analyze/traitement/Devoir_Python/venv/lib/python3.6/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             raise ValueError(\"n_samples=%d should be >= n_clusters=%d\" % (\n\u001b[0;32m--> 863\u001b[0;31m                 _num_samples(X), self.n_clusters))\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mtol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tolerance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: n_samples=1 should be >= n_clusters=2"
     ]
    }
   ],
   "source": [
    "#clusterise les documents, ici 8 clusters\n",
    "true_k = 2\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, on montre les termes que l'ordinateur a pu rassembler ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " polichinelle\n",
      " pierrot\n",
      " cassandre\n",
      " aller\n",
      " faire\n",
      " el\n",
      " monsieur\n",
      " gendarme\n",
      " qu\n",
      " niflanguille\n"
     ]
    }
   ],
   "source": [
    "#montre le top 10 des mot-clés les plus représentatifs de chaque cluster\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster de documents ##\n",
    "\n",
    "Grâce à un système de prédiction découlant de l'apprentissage et de l'entraînement sur les termes des abstracts, nous pouvons classifier les abstracts en 8 catégories. Nous pourrons ensuite les comparer avec le classement déjà fait par les organisateurs de la conférence TEI 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './cache2019/TEI2019.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-caa8cb8cb380>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtexte\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#je vectorise le texte de l'abstract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#j'attribue l'abstract à un cluster. predicted est de la classe numpy array : c'est un vecteur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./cache2019/TEI2019.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mtitre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './cache2019/TEI2019.csv'"
     ]
    }
   ],
   "source": [
    "Path = \"./cache2019/cacheLEM/\" #On peut remplacer le cache STEM par LEM à la place, pour ainsi comparer les deux sorties.\n",
    "filelist = os.listdir(Path) #filelist est une liste regroupant tous les chemins vers les différents abstracts.\n",
    "liste_triee = [] #c'est la liste contenant les infos triées de tous les abstracts\n",
    "\n",
    "\n",
    "for abstract in filelist:\n",
    "    reference = abstract.replace('.txt', '') #je normalise le nom du texte pour le faire correpondre avec celui indiqué dans le tableau csv réunissant toutes les informations\n",
    "    with open(Path + abstract, \"r\", encoding=\"UTF-8\") as y:\n",
    "        liste_resultat_unitaire = [] #j'instancie la liste des infos propres à chaque abstract\n",
    "        texte = y.read()\n",
    "        X = vectorizer.transform([texte]) #je vectorise le texte de l'abstract\n",
    "        predicted = model.predict(X)  #j'attribue l'abstract à un cluster. predicted est de la classe numpy array : c'est un vecteur\n",
    "        with open('./cache2019/TEI2019.csv', 'r') as csvfile:\n",
    "            read = csv.reader(csvfile, delimiter = ',')\n",
    "            titre = ''\n",
    "            auteurs = ''\n",
    "            for row in read:\n",
    "                if row[4] == reference:\n",
    "                    titre = row[2]\n",
    "                    auteurs = row[0]\n",
    "        liste_resultat_unitaire.append(titre)\n",
    "        liste_resultat_unitaire.append(auteurs)\n",
    "        liste_resultat_unitaire.append(int(predicted))\n",
    "        liste_triee.append(liste_resultat_unitaire)\n",
    "        \n",
    "list_grp1 = []\n",
    "list_grp2 = []\n",
    "list_grp3 = []\n",
    "list_grp4 = []\n",
    "list_grp5 = []\n",
    "list_grp6 = []\n",
    "list_grp7 = []\n",
    "list_grp8 = []\n",
    "\n",
    "print(\"Dans le groupe 1, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 0:\n",
    "        list_grp1.append(elem[0]) \n",
    "        print(\"\\t\", elem[0] )\n",
    "        \n",
    "print(\"\\n Dans le groupe 2, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 1:\n",
    "        list_grp2.append(elem[0])\n",
    "        print(\"\\t\", elem[0] )\n",
    "        \n",
    "print(\"\\n Dans le groupe 3, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 2:\n",
    "        list_grp3.append(elem[0])\n",
    "        print(\"\\t\", elem[0] )\n",
    "\n",
    "print(\"\\n Dans le groupe 4, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 3:\n",
    "        list_grp4.append(elem[0])\n",
    "        print(\"\\t\", elem[0] )\n",
    "\n",
    "print(\"\\n Dans le groupe 5, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 4:\n",
    "        list_grp5.append(elem[0])\n",
    "        print(\"\\t\", elem[0] )\n",
    "        \n",
    "print(\"\\n Dans le groupe 6, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 5:\n",
    "        list_grp6.append(elem[0])\n",
    "        print(\"\\t\", elem[0] )\n",
    "        \n",
    "print(\"\\n Dans le groupe 7, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 6:\n",
    "        list_grp7.append(elem[0])\n",
    "        print(\"\\t\", elem[0] )\n",
    "        \n",
    "print(\"\\n Dans le groupe 8, il y a :\")\n",
    "for elem in liste_triee:\n",
    "    if elem[2] == 7:\n",
    "        list_grp8.append(elem[0])\n",
    "        print(\"\\t\", elem[0] )\n",
    "\n",
    "\n",
    "#Des librairies python qui convertissent en table HMTL, faut que je fasse de la dataviz\n",
    "# chord visualisation est une librairie faisant des dataviz, Seaborn aussi, BOKEH aussi, Plotly (MatPlotLib est la base de tous), Parallel Plot est cool.\n",
    "# On peut utiliser XSLT via LXML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici le plan et l'agencement des conférences tel qu'il est indiqué ici : https://graz-2019.tei-c.org/wp-content/uploads/2019/09/ProgrammheftFINAL.pdf\n",
    "\n",
    "TEI, formal ontologies,controlled vocabularies and Linked OpenData :\n",
    "- Making Linkable Data from Account Books: Bookkeeping Ontology in the Digital Edition Publishing Cooperative for Historical Accounts\n",
    "- Inscriptions, Hieroglyphs, Linguistics and Beyond! The Corpus of Classic Mayan as an Ontological Information Resource\n",
    "- Modeling FRBR Entities and their Relationships with TEI: A Look at HallerNet Bibliographic Descriptions\n",
    "- Referencing an editorial ontology from the TEI: An attempt to overcome informal typologies\n",
    "- Text Graph Ontology. A Semantic Web approach to represent genetic scholarly editions\n",
    "\n",
    "TEI and models of text :\n",
    "- A realistic theory of textuality and its consequences on digital text representation\n",
    "- Genesis and Variance: From Letter to Literature\n",
    "- Between freedom and formalisation: a hypergraph model for representing the nature of text\n",
    "- Reflecting the Influence of Technology on Models of Text in Scholarly Digital Editing\n",
    "- Introducing Objectification: when is an <object> a <place> ?\n",
    "- An Encoding Strategic Proposal of “Ruby” Texts: Examples from Japanese Texts\n",
    "- Referencing annotations as a core concept of the hallerNet edition and research platform\n",
    "- Recreating history through events\n",
    "- Document Modeling with the TEI Critical Apparatus\n",
    "- Exploring TEI structures to find distinctive features of text types\n",
    "- Reconceiving TEI models of theatrical performance text with reference to promptbooks\n",
    "- What is a Line? Encoding and Counting Lines in Early Modern Dramatic Texts\n",
    "    \n",
    "TEI across corpora,languages, and cultures :\n",
    "- Growing collections of TEI texts: Some lessons from SARIT\n",
    "- Towards larger corpora of Indic texts: For now, minimize metatext\n",
    "- Encoding history in TEI: A corpus-oriented approach for investigating Tibetan historiography\n",
    "- Advantages and challenges of tokenized TEI\n",
    "- A sign of the times: medieval punctuation, its encoding and its rendition in modern times\n",
    "\n",
    "TEI annotation and publication :\n",
    "- Analyzing and Visualizing Uncertain Knowledge: Introducing the PROVIDEDH Open Science Platform\n",
    "- The Prefabricated Website: Who Needs a Server Anyway?\n",
    "- correspSearch v2 –New ways of exploring correspondence\n",
    "- Validating @selector: a regular expression adventure\n",
    "- TEI encoding of correspondence: A community effort\n",
    "\n",
    "TEI simplification and extension :\n",
    "- Opportunities and challenges of the TEI for scholarly journals in the Humanities\n",
    "- Archiving a TEI project FAIRly\n",
    "- Creating high-quality print from TEI documents\n",
    "- Native-TEI dialectal dictionary for Bavarian dialects in Austria: data structure, software and workflow\n",
    "- An Attempt of Dissemination of TEI in a TEI-underdeveloped country: Activities of the SIG EAJ\n",
    "- Refining the Current Teaching Methodology of the TEI through the Analysis of Server Logs\n",
    "- Using Github and its Integrations to Create, Test, and Deploy a Digital Edition\n",
    "\n",
    "TEI environments and infrastructures :\n",
    "- Parla-CLARIN: TEI guidelines for corpora of parliamentary proceedings\n",
    "- Challenges in encoding parliamentary data: between applause and interjections\n",
    "- A TEI customization for the description of paper and watermarks\n",
    "- How we tripled our encoding speed in the Digital Victorian Periodical Project\n",
    "- Manuscripta-The editor from past to future\n",
    "- Highlighting Our Examples: encoding XML examples in pedagogical contexts\n",
    "- Case Study TEI Customization: A Restricted TEI Format for Edition Open Access (EOA)\n",
    "- In search of comity: TEI for distant reading\n",
    "\n",
    "TEI and beyond :interactions, interchange, integrations and interoperability :\n",
    "- Using Machine Learning for the Automated Classification of Stage Directions in TEI-Encoded Drama Corpora\n",
    "- TEI XML and Delta Format Interchangeability\n",
    "\n",
    "TEI and non-XML technologies :\n",
    "- Five Centuries of History in a Network\n",
    "- Introducing an Open, Dynamic and Efficient Lexical Data Access for TEI-encoded Dictionaries on the Internet\n",
    "- Getting Along with Relational Databases\n",
    "- Using Microsoft Word for preparing XML TEI-compliant digital editions\n",
    "- Scaling up Automatic Structuring of Manuscript Sales Catalogues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'encode la répartition des titres dans les groupes dans des listes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_grp1 = ['Making Linkable Data from Account Books: Bookkeeping Ontology in the Digital Edition Publishing Cooperative for Historical Accounts',\n",
    "'Inscriptions, Hieroglyphs, Linguistics and Beyond! The Corpus of Classic Mayan as an Ontological Information Resource',\n",
    "'Modeling FRBR Entities and their Relationships with TEI: A Look at HallerNet Bibliographic Descriptions',\n",
    "'Referencing an editorial ontology from the TEI: An attempt to overcome informal typologies',\n",
    "'Text Graph Ontology. A Semantic Web approach to represent genetic scholarly editions']\n",
    "V_grp2 = ['A realistic theory of textuality and its consequences on digital text representation'\n",
    "'Genesis and Variance: From Letter to Literature',\n",
    "'Between freedom and formalisation: a hypergraph model for representing the nature of text',\n",
    "'Reflecting the Influence of Technology on Models of Text in Scholarly Digital Editing',\n",
    "'Introducing Objectification: when is an <object> a <place> ?',\n",
    "'An Encoding Strategic Proposal of “Ruby” Texts: Examples from Japanese Texts',\n",
    "'Referencing annotations as a core concept of the hallerNet edition and research platform',\n",
    "'Recreating history through events',\n",
    "'Document Modeling with the TEI Critical Apparatus',\n",
    "'Exploring TEI structures to find distinctive features of text types',\n",
    "'Reconceiving TEI models of theatrical performance text with reference to promptbooks',\n",
    "'What is a Line? Encoding and Counting Lines in Early Modern Dramatic Texts']\n",
    "V_grp3 = ['Growing collections of TEI texts: Some lessons from SARIT',\n",
    "'Towards larger corpora of Indic texts: For now, minimize metatext',\n",
    "'Encoding history in TEI: A corpus-oriented approach for investigating Tibetan historiography',\n",
    "'Advantages and challenges of tokenized TEI',\n",
    "'A sign of the times: medieval punctuation, its encoding and its rendition in modern times']\n",
    "V_grp4 = ['Analyzing and Visualizing Uncertain Knowledge: Introducing the PROVIDEDH Open Science Platform',\n",
    "'The Prefabricated Website: Who Needs a Server Anyway?',\n",
    "'correspSearch v2 –New ways of exploring correspondence',\n",
    "'Validating @selector: a regular expression adventure',\n",
    "'TEI encoding of correspondence: A community effort']\n",
    "V_grp5 = ['Opportunities and challenges of the TEI for scholarly journals in the Humanities',\n",
    "'Archiving a TEI project FAIRly',\n",
    "'Creating high-quality print from TEI documents',\n",
    "'Native-TEI dialectal dictionary for Bavarian dialects in Austria: data structure, software and workflow',\n",
    "'An Attempt of Dissemination of TEI in a TEI-underdeveloped country: Activities of the SIG EAJ',\n",
    "'Refining the Current Teaching Methodology of the TEI through the Analysis of Server Logs',\n",
    "'Using Github and its Integrations to Create, Test, and Deploy a Digital Edition']\n",
    "V_grp6 = ['Parla-CLARIN: TEI guidelines for corpora of parliamentary proceedings',\n",
    "'Challenges in encoding parliamentary data: between applause and interjections',\n",
    "'A TEI customization for the description of paper and watermarks',\n",
    "'How we tripled our encoding speed in the Digital Victorian Periodical Project',\n",
    "'Manuscripta-The editor from past to future',\n",
    "'Highlighting Our Examples: encoding XML examples in pedagogical contexts',\n",
    "'Case Study TEI Customization: A Restricted TEI Format for Edition Open Access (EOA)',\n",
    "'In search of comity: TEI for distant reading']\n",
    "V_grp7 = ['Using Machine Learning for the Automated Classification of Stage Directions in TEI-Encoded Drama Corpora',\n",
    "'TEI XML and Delta Format Interchangeability']\n",
    "V_grp8 = ['Five Centuries of History in a Network',\n",
    "'Introducing an Open, Dynamic and Efficient Lexical Data Access for TEI-encoded Dictionaries on the Internet',\n",
    "'Getting Along with Relational Databases',\n",
    "'Using Microsoft Word for preparing XML TEI-compliant digital editions',\n",
    "'Scaling up Automatic Structuring of Manuscript Sales Catalogues']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modélisation et comparaison ##\n",
    "\n",
    "En utilisant la librairie MatPlotLib, nous pouvons faire des graphiques permettant de mieux comprendre ce que nous avons créé. Nous comparerons les résultats obtenus avec les \" clusters \" créés par les organisateurs.\n",
    "\n",
    "### Un tableau ###\n",
    "\n",
    "Tout d'abord, nous allons représenter la liste des attributions sous forme d'un tableau pour comparer les clusters de l'ordinateur et les groupes faits par les organisateurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "headerColor = 'grey'\n",
    "rowEvenColor = 'lightgrey'\n",
    "rowOddColor = 'white'\n",
    "\n",
    "fig = go.Figure(data=[go.Table(\n",
    "  header=dict(\n",
    "    values=['<b>Groupe 1</b>','<b>Groupe 2</b>','<b>Groupe 3</b>','<b>Groupe 4</b>','<b>Groupe 5</b>','<b>Groupe 6</b>','<b>Groupe 7</b>','<b>Groupe 8</b>'],\n",
    "    line_color='darkslategray',\n",
    "    fill_color=headerColor,\n",
    "    align=['center'],\n",
    "    font=dict(color='white', size=12)\n",
    "  ),\n",
    "  cells=dict(\n",
    "    values=[\n",
    "      list_grp1,\n",
    "      list_grp2,\n",
    "      list_grp3,\n",
    "      list_grp4,\n",
    "      list_grp5,\n",
    "      list_grp6,\n",
    "      list_grp7,\n",
    "      list_grp8],\n",
    "    line_color='darkslategray',\n",
    "    # 2-D list of colors for alternating rows\n",
    "    fill_color = [[rowOddColor,rowEvenColor,rowOddColor, rowEvenColor,rowOddColor]*5],\n",
    "    align = ['center'],\n",
    "    font = dict(color = 'darkslategray', size = 9)\n",
    "    ))\n",
    "])\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure(data=[go.Table(\n",
    "  header=dict(\n",
    "    values=['<b>Groupe 1</b>','<b>Groupe 2</b>','<b>Groupe 3</b>','<b>Groupe 4</b>','<b>Groupe 5</b>','<b>Groupe 6</b>','<b>Groupe 7</b>','<b>Groupe 8</b>'],\n",
    "    line_color='darkslategray',\n",
    "    fill_color=headerColor,\n",
    "    align=['center'],\n",
    "    font=dict(color='white', size=12)\n",
    "  ),\n",
    "  cells=dict(\n",
    "    values=[\n",
    "      V_grp1,\n",
    "      V_grp2,\n",
    "      V_grp3,\n",
    "      V_grp4,\n",
    "      V_grp5,\n",
    "      V_grp6,\n",
    "      V_grp7,\n",
    "      V_grp8],\n",
    "    line_color='darkslategray',\n",
    "    # 2-D list of colors for alternating rows\n",
    "    fill_color = [[rowOddColor,rowEvenColor,rowOddColor, rowEvenColor,rowOddColor]*5],\n",
    "    align = ['center'],\n",
    "    font = dict(color = 'darkslategray', size = 9)\n",
    "    ))\n",
    "])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution à l'intérieur des clusters  ###\n",
    "\n",
    "Nous représentons la distribution des conférences dans un camembert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = 'Groupe 1', 'Groupe 2', 'Groupe 3', 'Groupe 4', 'Groupe 5', 'Groupe 6', 'Groupe 7', 'Groupe 8'\n",
    "sizes = [len(list_grp1), len(list_grp2), len(list_grp3), len(list_grp4), len(list_grp5), len(list_grp6), len(list_grp7), len(list_grp8)]\n",
    "colors = ['yellowgreen', 'gold', 'lightskyblue', 'lightcoral', 'blue', 'yellow', 'green', 'red']\n",
    "explode = (0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2)\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.axis('equal')\n",
    "plt.title(\"Regroupement des abstracts de la Conference TEI 2019 par thème selon l'ordinateur\")\n",
    "plt.show()\n",
    "\n",
    "labels = 'Groupe 1', 'Groupe 2', 'Groupe 3', 'Groupe 4', 'Groupe 5', 'Groupe 6', 'Groupe 7', 'Groupe 8'\n",
    "sizes = [len(V_grp1), len(V_grp2), len(V_grp3), len(V_grp4), len(V_grp5), len(V_grp6), len(V_grp7), len(V_grp8)]\n",
    "colors = ['yellowgreen', 'gold', 'lightskyblue', 'lightcoral', 'blue', 'yellow', 'green', 'red']\n",
    "explode = (0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2)\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.axis('equal')\n",
    "plt.title(\"Regroupement des abstracts de la Conference TEI 2019 par thème selon les organisateurs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voyons que selon l'ordinateur il existe un gros ensemble massif regroupant au moins la moitié des abstracts, et les autres abstracts sont périphériques. Pour l'ordinateur, la quasi-totalité des conférences seraient donc sur un thème commun. Cela semble cohérent (en effet, c'est une conférence de TEI...), mais peu intéressant.\n",
    "\n",
    "Par ailleurs, il est à noter que si l'on effectue un certain nombre de reboot et de lancement, les résultats diffèrent sensiblement. La part de l'aléatoire semble donc prédominer, et la machine semble n'être pas tellement fiable. \n",
    "\n",
    "A l'inverse, la répartition des organisateurs est plus ou moins égale, ce qui s'explique par exemple par le besoin d'équilibrer les journées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprendre ses clusters avec NMF ###\n",
    "\n",
    "Grâce à la librairie NMF (Factorisation Matricielle Non-négative), je peux essayer de comprendre les clusters produits pour la machine en lui demandant de sortir les mots les plus représentatifs de chaque cluster. \n",
    "\n",
    "Nous allons donc pouvoir utiliser la librairie NMF qui se base sur la vectorisation de SKLearn pour reconstruire les clusters, mais cette fois-ci en sortant les mot-clés les plus utilisés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer(smooth_idf=True)\n",
    "x_tfidf = transformer.fit_transform(X) #j'insère X, c'est à dire les documents vectorisés\n",
    "xtfidf_norm = normalize(x_tfidf, norm='l1', axis=1) #normalisation des vecteurs pour rentrer dans la matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 8\n",
    "modelnmf = NMF(n_components=num_topics) #Je construis l'algorithme de NMF\n",
    "modelnmf.fit(xtfidf_norm) #J'entre mes vecteurs normalisés dans l'algorithme de NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nmf_topics(model, n_top_words):\n",
    "    \n",
    "    # Je récupère ici un dictionnaire avec le mot en clé et son vecteur en valeur. Permet de récupérer les mots.\n",
    "    feat_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    word_dict = {}\n",
    "    for i in range(num_topics):\n",
    "        \n",
    "        #Pour chaque cluster j'obtiens les plus gros vecteurs, et j'ajoute les mots dans le dictionnaires initialisés \"word_dict\".\n",
    "        words_ids = modelnmf.components_[i].argsort()[:-20 - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Cluster n° ' + '{:02d}'.format(i+1)] = words\n",
    "    \n",
    "    return pd.DataFrame(word_dict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_nmf_topics(modelnmf, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les clusters ainsi formés ont, comme on aurait pu l'imaginer, aucun sens pour un être humain malheureusement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
