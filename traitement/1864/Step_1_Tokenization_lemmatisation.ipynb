{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22 textes pour marionnettes de Duranty #\n",
    "\n",
    "Nous allons tout d'abord copier les fichiers textes issus de l'océrisation dans le cache. Les fichiers textes contiennent chacun une pièce. Le découpage a été fait à la main, l'océrisation aussi.\n",
    "\n",
    "## Etape 1 : Tokenisation et Lemmatisation ##\n",
    "\n",
    "Nous allons ici créer des documents lemmatisés à partir d'un fichier en texte brut.\n",
    "\n",
    "Pour cela, il s'agira d'abord de tokenizer chaque mot, puis de reproduire chaque abstract où chaque mot sera remplacé par son lemme. Cela permet ainsi de réduire le bruit et les erreurs lors de l'analyse du document par la machine, et faciliter ainsi le rapprochement des documents pour former des clusters.\n",
    "\n",
    "Nous offrons cependant 2 alternatives à l'analyse :\n",
    "- La lemmatisation, produisant une version décrétée canonique de chaque mot en reconnaissant sa forme.\n",
    "- La stemmisation, retirant la terminaison des mots pour ne garder que le radical.\n",
    "\n",
    "### Les packages ###\n",
    "\n",
    "Nous allons utiliser notamment les packages nltk, spaCy et PorterStemmer pour produire des documents lemmatisés et stemmisés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import os\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "from shutil import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Méthode 1 : tokenisation et lemmatisation ####\n",
    "\n",
    "Je propose ci-dessous de lemmatizer le texte avec spaCy. Ma lemmatisation va retourner une forme canonique de chaque mot. La tokenisation et la lemmatisation s'appuient sur un petit corpus que j'ai importé (en_core_web_sm) depuis la base de spaCy. En entrée se trouve le texte normalisé de chaque abstract, et en sortie nous retrouvons ce même texte, mais où chaque mot est remplacé par son lem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./cache1864/\"): #permet de créer le cache s'il est supprimé.\n",
    "    os.makedirs(\"./cache1864/\")\n",
    "\n",
    "try: #cela permet de récupérer les ficheirs textes depuis le début de l'arborescence pour les copier dans le cache. c'est plus simple à manipuler\n",
    "    source_ocr_parent = Path().resolve().parent.parent #je remonte l'arborescence jusque là où se trouvent les fichiers\n",
    "    source_ocr = str(source_ocr_parent) + \"/1864_segmente/\" #je redescends dans mon arbre\n",
    "    shutil.copytree(dd,\"./cache1864/cacheTXT\") #je copie\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = \"./cache1864/cacheTXT/\" #chemin permettant d'accès aux abstracts\n",
    "Path_output = \"./cache1864/cacheLEM/\" #chemin de sortie des abstracts stemmés\n",
    "filelist = os.listdir(Path) #filelist est une liste regroupant tous les chemins vers les différents abstracts.\n",
    "if not os.path.exists(\"./cache1864/cacheLEM/\"): #permet de créer un dossier dans le cache s'il est supprimé.\n",
    "    os.makedirs(\"./cache1864/cacheLEM/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for abstract in filelist:\n",
    "    with open(Path + abstract, \"r\", encoding=\"UTF-8\") as y:\n",
    "        texte = y.read()\n",
    "        nlp = spacy.load(\"fr_core_news_sm\") #je récupère le corpus français de spacy\n",
    "        doc = nlp(texte) #doc est le texte annoté. Ce n'est pas une str pour autant\n",
    "        liste_mots_lemmatise = [] # j'instancie une liste qui accueillera chaque lemme (le lemme étant une str)\n",
    "        for token in doc:\n",
    "            liste_mots_lemmatise.append(token.lemma_) #token.lemma_ est une str. Cette méthode tokenise puis lematise. Il crée une liste qu'on va joindre ensuite.\n",
    "        resultat = ' '.join(liste_mots_lemmatise)\n",
    "        with open(Path_output + abstract, \"w\", encoding=\"UTF-8\") as z:\n",
    "            z.write(resultat) #resultat est une str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Méthode 2 : tokenization + stemmisation ####\n",
    "\n",
    "Dans cette seconde méthode, le texte est tokenisé (grâce à spaCy toujours) mais il est stemmé et non pas lemmatisé. Le stemming consiste à retirer les terminaisons pour garder une forme canonique de chaque mot. Le fonctionnement est identique à celui de la lemmatisation, pour ma démarche. Le résultat est également stocké dans un cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = \"./cache1864/cacheTXT/\" #chemin permettant d'accès aux abstracts\n",
    "Path_output = \"./cache1864/cacheSTEM/\" #chemin de sortie des abstracts stemmés\n",
    "filelist = os.listdir(Path) #filelist est une liste regroupant tous les chemins vers les différents abstracts.\n",
    "if not os.path.exists(\"./cache1864/cacheSTEM/\"): #permet de créer un dossier dans le cache s'il est supprimé.\n",
    "    os.makedirs(\"./cache1864/cacheSTEM/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer() #J'importe une méthode acceptant une chaîne de caractère à une variable.\n",
    "for abstract in filelist:\n",
    "    with open(Path + abstract, \"r\", encoding=\"UTF-8\") as y:\n",
    "        texte = y.read()\n",
    "        liste_mots_tokenise = [] # j'instancie une liste qui accueillera chaque lemme (le lemme étant une str)\n",
    "        liste_mots_a_tokenise = texte.split() #je crée une liste sur laquelle je vais pouvoir boucler\n",
    "        for elem in liste_mots_a_tokenise:\n",
    "            mot_tokenise = stemmer.stem(elem) #mot_tokenise est une str, et est le lemme du mot sur lequel je boucle\n",
    "            liste_mots_tokenise.append(mot_tokenise) #j'ajoute chaque mot à une liste, qu'ensuite je join pour recréer l'abstract sous la forme d'une str\n",
    "        resultat = ' '.join(liste_mots_tokenise)\n",
    "        with open(Path_output + abstract, \"w\", encoding=\"UTF-8\") as z:\n",
    "            z.write(resultat) #resultat est une str"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
